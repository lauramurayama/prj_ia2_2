{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bddded0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Social Media Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1bb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Downloads\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4134a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy with the new park.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feeling sad about the pollution.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy with the new park.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The weather is amazing.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The public services are great.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text sentiment\n",
       "0          Happy with the new park.   neutral\n",
       "1  Feeling sad about the pollution.   neutral\n",
       "2          Happy with the new park.   neutral\n",
       "3           The weather is amazing.   neutral\n",
       "4    The public services are great.   neutral"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função de pré-processamento\n",
    "def preprocess(text):\n",
    "    text = str(text)\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r'https?://\\S+|@\\w+|#[\\w-]+', '', text)  # Remove links, menções, hashtags\n",
    "    text = re.sub(r'\\W|\\d', ' ', text.lower())\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Carregar dados\n",
    "df = pd.read_csv(\"improved_sentiment_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0e0fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                 sublinear_tf=True)),\n",
       "                (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                 sublinear_tf=True)),\n",
       "                (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2), sublinear_tf=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2),\n",
       "                                 sublinear_tf=True)),\n",
       "                ('smote', SMOTE(random_state=42)),\n",
       "                ('clf', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pré-processar\n",
    "df.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "df['text_clean'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Codificar rótulos\n",
    "le = LabelEncoder()\n",
    "df['sentiment_encoded'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_clean'], df['sentiment_encoded'], test_size=0.2, stratify=df['sentiment_encoded'], random_state=42\n",
    ")\n",
    "\n",
    "# Vetorizador separado\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "smote_step = SMOTE(random_state=42) \n",
    "vectorizer_for_pipeline = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.85, sublinear_tf=True)\n",
    "model_for_pipeline = LogisticRegression(max_iter=1000)\n",
    "\n",
    "final_pipeline = ImbPipeline([ \n",
    "     ('tfidf', vectorizer_for_pipeline),\n",
    "     ('smote', smote_step), \n",
    "     ('clf', model_for_pipeline)\n",
    " ])\n",
    "\n",
    "final_pipeline.fit(df['text_clean'], df['sentiment_encoded']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e52177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Modelo salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvar modelo e label encoder\n",
    "joblib.dump(final_pipeline, 'sentiment_model.joblib')\n",
    "joblib.dump(le, 'label_encoder.joblib')\n",
    "print(\"\\n✅ Modelo salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c024ba",
   "metadata": {},
   "source": [
    "# Análise de Sentimento: Técnicas Implementadas no Código\n",
    "\n",
    "O código Python fornecido implementa um fluxo completo de Machine Learning para análise de sentimento em texto. Abaixo estão as técnicas utilizadas, descritas em detalhes:\n",
    "\n",
    "### 1. Importações de Bibliotecas\n",
    "\n",
    "* **`pandas`**: Usada para manipulação e análise de dados tabulares (DataFrames).\n",
    "* **`numpy`**: Fornece suporte para arrays e operações numéricas eficientes.\n",
    "* **`re`**: Módulo para operações com expressões regulares, essencial para a limpeza de texto.\n",
    "* **`nltk`**: (Natural Language Toolkit) Biblioteca para processamento de linguagem natural.\n",
    "* **`joblib`**: Usada para salvar e carregar modelos treinados e outros objetos Python.\n",
    "* **`seaborn` e `matplotlib.pyplot`**: Bibliotecas para visualização de dados, especialmente para a matriz de confusão.\n",
    "* **`unidecode`**: Converte caracteres acentuados ou especiais para sua representação ASCII mais próxima (ex: \"ação\" para \"acao\").\n",
    "* **`nltk.corpus.stopwords`**: Contém uma lista de palavras comuns (stopwords) que são frequentemente removidas do texto.\n",
    "* **`sklearn.model_selection.train_test_split`**: Divide o conjunto de dados em subconjuntos de treinamento e teste.\n",
    "* **`sklearn.preprocessing.LabelEncoder`**: Converte rótulos categóricos em rótulos numéricos (0, 1, 2...).\n",
    "* **`sklearn.pipeline.Pipeline`**: Permite encadear várias etapas de processamento de dados e modelagem em um único objeto, garantindo que as transformações sejam aplicadas consistentemente.\n",
    "* **`sklearn.feature_extraction.text.TfidfVectorizer`**: Converte uma coleção de documentos brutos em uma matriz de recursos TF-IDF.\n",
    "* **`sklearn.metrics`**: Módulo que oferece diversas métricas para avaliar o desempenho do modelo (accuracy_score, classification_report, confusion_matrix, f1_score).\n",
    "* **`sklearn.linear_model.LogisticRegression`**: Implementa o algoritmo de Regressão Logística, um modelo linear para classificação.\n",
    "* **`imblearn.over_sampling.SMOTE`**: (Synthetic Minority Over-sampling Technique) Técnica de reamostragem para lidar com desequilíbrio de classes.\n",
    "\n",
    "### 2. Download de Dados NLTK\n",
    "\n",
    "* **`nltk.download('stopwords')`**: Baixa as palavras irrelevantes (stopwords) para vários idiomas, neste caso, português e inglês.\n",
    "* **`stop_words = set(stopwords.words('portuguese')).union(set(stopwords.words('english')))`**: Cria um conjunto único de stopwords em português e inglês para remoção eficiente. Usar um `set` melhora a performance da busca.\n",
    "\n",
    "### 3. Pré-processamento de Texto (`preprocess` function)\n",
    "\n",
    "Esta função é crucial para limpar e normalizar o texto antes da vetorização:\n",
    "\n",
    "* **`text = str(text)`**: Garante que a entrada seja uma string.\n",
    "* **`text = unidecode(text)`**: Remove acentos e caracteres especiais, padronizando o texto (ex: \"Olá\" -> \"Ola\").\n",
    "* **`text = re.sub(r'https?://\\S+|@\\w+|#[\\w-]+', '', text)`**: Remove padrões comuns em texto de redes sociais:\n",
    "    * `https?://\\S+`: URLs (links).\n",
    "    * `@\\w+`: Menções de usuários (ex: `@fulano`).\n",
    "    * `#[\\w-]+`: Hashtags (ex: `#exemplo`).\n",
    "* **`text = re.sub(r'\\W|\\d', ' ', text.lower())`**:\n",
    "    * `text.lower()`: Converte todo o texto para minúsculas, garantindo que \"Amor\" e \"amor\" sejam tratados como a mesma palavra.\n",
    "    * `re.sub(r'\\W|\\d', ' ', ...)`: Remove caracteres não alfanuméricos (`\\W`) e dígitos (`\\d`), substituindo-os por um espaço. Isso ajuda a isolar as palavras.\n",
    "* **`tokens = text.split()`**: Divide o texto em uma lista de palavras (tokens).\n",
    "* **`tokens = [t for t in tokens if t not in stop_words and len(t) > 2]`**: Filtra os tokens:\n",
    "    * `t not in stop_words`: Remove palavras comuns que não adicionam significado ao sentimento (ex: \"o\", \"a\", \"de\", \"e\").\n",
    "    * `len(t) > 2`: Remove palavras muito curtas (geralmente ruído ou caracteres residuais).\n",
    "* **`return ' '.join(tokens)`**: Junta os tokens limpos de volta em uma única string.\n",
    "\n",
    "### 4. Carregamento e Preparação Inicial dos Dados\n",
    "\n",
    "* **`df = pd.read_csv(\"improved_sentiment_data.csv\")`**: Carrega os dados de um arquivo CSV.\n",
    "* **`df.dropna(subset=['text', 'sentiment'], inplace=True)`**: Remove linhas onde as colunas 'text' ou 'sentiment' possuem valores nulos. Isso é uma etapa importante de **limpeza de dados**.\n",
    "* **`df['text_clean'] = df['text'].apply(preprocess)`**: Aplica a função de pré-processamento `preprocess` a cada texto na coluna 'text', criando uma nova coluna `text_clean` com os textos limpos.\n",
    "\n",
    "### 5. Codificação de Rótulos (Label Encoding)\n",
    "\n",
    "* **`le = LabelEncoder()`**: Inicializa o codificador de rótulos.\n",
    "* **`df['sentiment_encoded'] = le.fit_transform(df['sentiment'])`**: Converte os rótulos de sentimento categóricos (ex: 'positivo', 'negativo', 'neutro') em valores numéricos inteiros (ex: 0, 1, 2). Isso é necessário porque os algoritmos de Machine Learning geralmente trabalham com entradas numéricas. `fit_transform` aprende os mapeamentos e os aplica.\n",
    "\n",
    "### 6. Divisão de Dados (Train-Test Split)\n",
    "\n",
    "* **`X_train, X_test, y_train, y_test = train_test_split(...)`**: Divide o conjunto de dados em subconjuntos para treinamento e teste.\n",
    "    * `df['text_clean']`: Os dados de entrada (features) para o modelo.\n",
    "    * `df['sentiment_encoded']`: Os rótulos de saída (target).\n",
    "    * `test_size=0.2`: 20% dos dados serão usados para teste, 80% para treinamento.\n",
    "    * **`stratify=df['sentiment_encoded']`**: Esta é uma técnica crucial para garantir que a proporção de classes no conjunto de treinamento e teste seja a mesma que no conjunto de dados original. Isso é especialmente importante em conjuntos de dados desbalanceados para evitar que um dos conjuntos tenha poucas ou nenhuma amostra de uma classe minoritária.\n",
    "    * `random_state=42`: Garante a reprodutibilidade da divisão.\n",
    "\n",
    "### 7. Vetorização de Texto (TF-IDF)\n",
    "\n",
    "* **`vectorizer = TfidfVectorizer(...)`**: Transforma o texto limpo em representações numéricas que o modelo pode entender.\n",
    "    * **`ngram_range=(1, 2)`**: Inclui tanto palavras únicas (unigrams) quanto pares de palavras consecutivas (bigrams) como recursos. Bigrams podem capturar mais contexto (ex: \"não gosto\" é diferente de \"gosto\").\n",
    "    * **`min_df=2`**: Ignora termos que aparecem em menos de 2 documentos. Isso ajuda a remover palavras muito raras que podem ser ruído ou irrelevantes.\n",
    "    * **`max_df=0.85`**: Ignora termos que aparecem em mais de 85% dos documentos. Isso ajuda a remover palavras muito comuns (mesmo que não sejam stopwords) que podem não ter poder discriminatório (ex: \"o\" ou \"e\" se não foram totalmente removidos).\n",
    "    * **`sublinear_tf=True`**: Aplica uma escala logarítmica à frequência de termos (TF), o que significa que o aumento da frequência de uma palavra tem um impacto menor nas pontuações TF-IDF. Ajuda a reduzir o impacto de palavras muito frequentes.\n",
    "* **`X_train_vec = vectorizer.fit_transform(X_train)`**: `fit_transform` aprende o vocabulário e os pesos TF-IDF do conjunto de treinamento e depois o transforma.\n",
    "* **`X_test_vec = vectorizer.transform(X_test)`**: `transform` aplica o vocabulário e os pesos TF-IDF *aprendidos no conjunto de treinamento* ao conjunto de teste. É fundamental não usar `fit_transform` no conjunto de teste para evitar vazamento de dados (data leakage).\n",
    "\n",
    "### 8. Balanceamento de Classes (SMOTE)\n",
    "\n",
    "* **`smote = SMOTE(random_state=42)`**: Inicializa o algoritmo SMOTE.\n",
    "* **`X_train_bal, y_train_bal = smote.fit_resample(X_train_vec, y_train)`**: Aplica SMOTE ao conjunto de treinamento. Esta é uma técnica de **oversampling**, que gera amostras sintéticas da(s) classe(s) minoritária(s) para balancear a distribuição de classes. Isso é crucial para modelos de classificação em dados desbalanceados, pois ajuda o modelo a aprender igualmente sobre todas as classes, evitando o viés em direção à classe majoritária.\n",
    "\n",
    "### 9. Treinamento do Modelo (Regressão Logística)\n",
    "\n",
    "* **`model = LogisticRegression(max_iter=1000)`**: Inicializa o modelo de Regressão Logística.\n",
    "    * **`max_iter=1000`**: Define o número máximo de iterações para o algoritmo de otimização. Um valor maior pode ajudar o modelo a convergir se os dados forem complexos, mas também aumenta o tempo de treinamento.\n",
    "* **`model.fit(X_train_bal, y_train_bal)`**: Treina o modelo usando os dados de treinamento vetorizados e balanceados.\n",
    "\n",
    "### 10. Avaliação do Modelo\n",
    "\n",
    "* **`y_pred = model.predict(X_test_vec)`**: Realiza previsões no conjunto de teste.\n",
    "* **`accuracy = accuracy_score(y_test, y_pred)`**: Calcula a **acurácia**, que é a proporção de previsões corretas.\n",
    "* **`f1 = f1_score(y_test, y_pred, average='weighted')`**: Calcula o **F1-score**.\n",
    "    * O F1-score é a média harmônica de precisão e recall. É uma métrica mais robusta que a acurácia para conjuntos de dados desbalanceados.\n",
    "    * `average='weighted'`: Calcula o F1-score para cada classe e depois pondera pela proporção de cada classe no conjunto de dados.\n",
    "* **`conf_matrix = confusion_matrix(y_test, y_pred)`**: Gera a **matriz de confusão**, que mostra o número de verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos. É fundamental para entender o desempenho do modelo em cada classe.\n",
    "* **`class_report = classification_report(y_test, y_pred, target_names=le.classes_)`**: Gera um **relatório de classificação** detalhado, incluindo precisão (precision), recall, F1-score e suporte (número de ocorrências) para cada classe. `target_names=le.classes_` exibe os nomes originais dos sentimentos em vez de 0, 1, 2.\n",
    "\n",
    "### 11. Salvar o Modelo e o Codificador (Pipeline)\n",
    "\n",
    "* **`final_pipeline = Pipeline([('tfidf', vectorizer), ('clf', model)])`**: Cria um `Pipeline` que encapsula tanto a etapa de vetorização TF-IDF quanto o modelo de Regressão Logística. Isso é uma **prática recomendada** porque garante que, ao usar o modelo no futuro, as mesmas transformações de pré-processamento e vetorização aplicadas aos dados de treinamento sejam aplicadas aos novos dados de entrada.\n",
    "* **`final_pipeline.fit(df['text_clean'], df['sentiment_encoded'])`**: Treina o pipeline completo com *todos* os dados disponíveis (`df['text_clean']` e `df['sentiment_encoded']`). Isso é comum após a avaliação inicial para maximizar o uso dos dados no modelo final para produção.\n",
    "* **`joblib.dump(final_pipeline, 'sentiment_model.joblib')`**: Salva o pipeline treinado em um arquivo para que possa ser carregado e usado posteriormente sem a necessidade de retreinamento.\n",
    "* **`joblib.dump(le, 'label_encoder.joblib')`**: Salva o `LabelEncoder` para que os rótulos numéricos previstos pelo modelo possam ser convertidos de volta aos seus nomes originais (ex: 0 -> 'positivo').\n",
    "\n",
    "### 12. Visualização (Matriz de Confusão)\n",
    "\n",
    "* **`sns.heatmap(...)`**: Utiliza a biblioteca `seaborn` para criar um mapa de calor visual da matriz de confusão.\n",
    "    * `annot=True`: Mostra os valores numéricos em cada célula.\n",
    "    * `fmt=\"d\"`: Formata os números como inteiros.\n",
    "    * `cmap=\"Blues\"`: Define o esquema de cores.\n",
    "    * `xticklabels=le.classes_`, `yticklabels=le.classes_`: Define os rótulos dos eixos com os nomes originais dos sentimentos.\n",
    "* **`plt.xlabel(...)`, `plt.ylabel(...)`, `plt.title(...)`**: Adiciona rótulos e um título ao gráfico para melhor clareza.\n",
    "* **`plt.show()`**: Exibe o gráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a63676",
   "metadata": {},
   "source": [
    "# Próximos Passos para Melhoria do Modelo de Análise de Sentimento\n",
    "\n",
    "### 1. Refinamento da Engenharia de Features e Pré-processamento\n",
    "\n",
    "* **Melhorar a base:** Essa base de dados foi gerada artificialmente, o que pode diminuir o desempenho dos modelos.\n",
    "* **Expandir Stopwords:** A lista de stopwords pode ser expandida com termos específicos do domínio (gírias, abreviações comuns em redes sociais que não carregam sentimento, etc.).\n",
    "* **Correção Ortográfica:** Implementar um passo de correção ortográfica pode ajudar a padronizar o vocabulário e reduzir o ruído, especialmente em textos com erros de digitação.\n",
    "* **Análise de Sentimento Negada:** A atual remoção de stopwords pode remover palavras de negação (ex: \"não gosto\"). Modificar o pré-processamento para manter e associar o \"não\" à palavra seguinte (ex: \"não_gosto\") pode ser benéfico.\n",
    "* **Remoção de Pontuação e Caracteres Especiais Otimizada:** Embora já esteja sendo feito, revisar se há padrões de pontuação ou caracteres que possam ter significado contextual e estejam sendo removidos indevidamente.\n",
    "* **Stemming/Lematização:**\n",
    "    * **Stemming:** Reduz palavras à sua raiz (ex: \"correndo\", \"corria\", \"corre\" -> \"corr\"). Mais agressivo e pode gerar \"raízes\" que não são palavras reais.\n",
    "    * **Lematização:** Reduz palavras à sua forma base (lema), que é uma palavra real (ex: \"amando\", \"amava\" -> \"amar\"). Geralmente mais preciso, mas computacionalmente mais caro. Pode ajudar a reduzir a dimensionalidade e agrupar termos com o mesmo significado.\n",
    "* **Vetorização Avançada:**\n",
    "    * **Ajustar `TfidfVectorizer`:** Experimentar diferentes valores para `min_df`, `max_df`, `ngram_range`. Por exemplo, `ngram_range=(1,3)` para incluir trigrams.\n",
    "    * **Word Embeddings (Word2Vec, GloVe, FastText):** Em vez de TF-IDF, que é uma representação esparsa, usar embeddings de palavras densos pode capturar melhor as relações semânticas entre as palavras. Pode-se usar embeddings pré-treinados ou treinar os próprios.\n",
    "    * **Embeddings Contextuais (BERT, ELMo, GPT):** Para um nível ainda mais avançado, utilizar modelos de linguagem pré-treinados (como BERT ou modelos da família GPT) e fazer fine-tuning para a tarefa de classificação de sentimento. Estes modelos capturam o contexto da palavra na frase, o que é um avanço significativo.\n",
    "\n",
    "### 2. Experimentação com Outros Modelos\n",
    "\n",
    "* **Modelos de Boosting (XGBoost, LightGBM, CatBoost):** São algoritmos ensemble potentes e frequentemente alcançam alta performance em dados tabulares e esparsos como TF-IDF.\n",
    "* **Redes Neurais:**\n",
    "    * **Redes Neurais Recorrentes (RNNs, LSTMs, GRUs):** Boas para dados sequenciais como texto, pois podem capturar dependências de longo alcance.\n",
    "    * **Redes Neurais Convolucionais (CNNs para texto):** Podem capturar padrões locais (frases curtas) no texto.\n",
    "    * **Modelos Baseados em Transformers (com fine-tuning):** Para o estado da arte em PNL, o fine-tuning de modelos como BERT, RoBERTa, ou XLM-R (para multi-idioma) é a abordagem mais poderosa, mas exige mais recursos computacionais.\n",
    "\n",
    "### 3. Otimização e Validação do Modelo\n",
    "\n",
    "* **Otimização de Hiperparâmetros:**\n",
    "    * **Grid Search/Random Search:** Já mencionadas, mas aplicá-las a novos modelos e aos hiperparâmetros do `TfidfVectorizer` (ex: `min_df`, `max_df`, `ngram_range`) e do modelo de classificação.\n",
    "    * **Otimização Bayesiana:** Mais eficiente que Grid/Random Search para encontrar os melhores hiperparâmetros.\n",
    "* **Validação Cruzada (Cross-Validation):** Embora a divisão treino-teste seja um bom começo, aplicar validação cruzada k-fold no treinamento final (após o SMOTE, se aplicado separadamente no `X_train_vec`) dará uma estimativa mais robusta da performance do modelo.\n",
    "* **Análise de Desempenho por Classe:** A matriz de confusão e o relatório de classificação já fornecem isso. Entender quais classes o modelo está tendo mais dificuldade em prever (ex: confunde \"neutro\" com \"negativo\") pode direcionar as melhorias.\n",
    "\n",
    "### 4. Robustez e Monitoramento em Produção\n",
    "\n",
    "* **Validação do Modelo:** Desenvolver um conjunto de validação separado (além do teste) para ajuste de hiperparâmetros e *early stopping* (se aplicável ao modelo escolhido).\n",
    "* **Testes de Robustez:**\n",
    "    * **Injeção de Ruído:** Testar o modelo com variações ligeiras no texto (erros de digitação comuns, abreviações) para ver como ele se comporta.\n",
    "    * **Teste de Adversarial Examples:** Embora mais avançado, tentar gerar exemplos que enganem o modelo.\n",
    "* **Interpretabilidade:** Usar técnicas como LIME ou SHAP para entender quais palavras/recursos o modelo está usando para fazer suas previsões, o que pode revelar vieses ou erros.\n",
    "* **Monitoramento Contínuo:** Se o modelo for para produção, é crucial monitorar seu desempenho ao longo do tempo. O vocabulário e a linguagem podem mudar (deriva de dados), e o modelo pode precisar ser retreinado periodicamente.\n",
    "\n",
    "### 5. Considerações sobre Desequilíbrio de Classes\n",
    "\n",
    "* **Experimentar Outras Técnicas de Balanceamento:** \n",
    "    * **ADASYN:** Semelhante ao SMOTE, mas foca em amostras que são mais difíceis de classificar.\n",
    "    * **Undersampling:** Reduzir o número de amostras da classe majoritária (pode perder informações).\n",
    "    * **Combinação de Oversampling e Undersampling:** Estratégias híbridas.\n",
    "    * **Ajuste de Pesos de Classe no Modelo:** Muitos modelos (como `LogisticRegression` e `SVC`) permitem atribuir pesos diferentes às classes durante o treinamento para dar mais importância à classe minoritária. Isso pode ser mais simples que o SMOTE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
